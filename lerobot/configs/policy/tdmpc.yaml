# @package _global_


policy:
  name: tdmpc

  # Input / output structure.
  n_action_repeats: 2
  horizon: 5

  input_shapes:
    # TODO(rcadene, alexander-soare): add variables for height and width from the dataset/env?
    observation.image: [3, 84, 84]
    observation.state: ["${env.state_dim}"]
  output_shapes:
    action: ["${env.action_dim}"]

  # Architecture / modeling.
  # Neural networks.
  image_encoder_hidden_dim: 32
  state_encoder_hidden_dim: 256
  latent_dim: 50
  q_ensemble_size: 5
  mlp_dim: 512
  # Reinforcement learning.
  discount: 0.9

  # Inference.
  use_mpc: true
  cem_iterations: 6

  num_samples: 512
  num_elites: 50
  mixture_coef: 0.1
  min_std: 0.05
  max_std: 2.0
  temperature: 0.5
  momentum: 0.1
  uncertainty_cost: 1

  # actor
  log_std_min: -10
  log_std_max: 2

  # learning
  reward_coef: 0.5
  value_coef: 0.1
  consistency_coef: 20
  pi_coef: 0.5
  rho: 0.5

  ema_alpha: 0.995


  # offline rl
  # dataset_dir: ???
  data_first_percent: 1.0
  is_data_clip: true
  data_clip_eps: 1e-5
  expectile: 0.9
  A_scaling: 3.0

  # offline->online
  offline_steps: ${offline_steps}
  pretrained_model_path: ""
  # pretrained_model_path: "/home/rcadene/code/fowm/logs/xarm_lift/all/default/2/models/offline.pt"
  # pretrained_model_path: "/home/rcadene/code/fowm/logs/xarm_lift/all/default/2/models/final.pt"
  balanced_sampling: true
  demo_schedule: 0.5

  # ---
  # TODO(alexander-soare): Remove these from the policy config.
  batch_size: 256
  grad_clip_norm: 10.0
  lr: 3e-4
  utd: 1


  delta_timestamps:
    observation.image: "[i / ${fps} for i in range(${policy.horizon} + 1)]"
    observation.state: "[i / ${fps} for i in range(${policy.horizon} + 1)]"
    action: "[i / ${fps} for i in range(${policy.horizon})]"
    next.reward: "[i / ${fps} for i in range(${policy.horizon})]"
